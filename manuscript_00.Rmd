---
title: 'An Million Transcriptome Ocular Meta-Atlas for Discovery and Development'
author:
  - Vinay Swamy:
      institute: bg
  - Temesgen Fufa:
      institute: mgog
  - Robert Hufnagel:
      institute: mgog
  - David McGaughey:
      institute:
        - bg
      correspondence: "yes"
      email: mcgaugheyd@mail.nih.gov
institute:
  - bg: Bioinformatics Group, Ophthalmic Genetics & Visual Function Branch, National Eye Institute, National Institutes of Health
  - mgog: Medical Genetics and Ophthalmic Genomics Unit, National Eye Institute, National Institutes of Health
    
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  word_document:
    reference_docx: word-styles-reference-01.docx
    fig_caption: yes
    pandoc_args:
      - '--lua-filter=scholarly-metadata.lua'
      - '--lua-filter=author-info-blocks.lua'
bibliography: massive_integrated_eye_scRNA.bib
csl: investigative-ophthalmology-and-visual-science.csl
abstract: "The development of highly scalable single cell transcriptome technology has allowed for a high resolution view into huge numbers of single cell transcriptomes. Analyzing the transcriptomes between different projects is highly desirable as this would theoretically allow for better assessment of which effects are consistent across independent studies. However it is difficult to compare and contrast data across different projects as there are substantial batch effects from computational processing, single cell technology utilized, and the natural biological variation. While many single cell transcriptome specific batch correction methods purport to remove the technical noise it is difficult to ascertain which method works best. We developed an R package (scPOP) that can scan the results of multiple methods and key parameter choices and create a ranked list of the best performing method / parameter combinations. We use this package along with a Snakefile based workflow system to demonstrate how to optimally merge over 1.4 million cells from 30 datsets and three species to create a massive ocular single cell transcriptome meta-atlas. This provides a model how to efficiently create meta-atlases for tissues and cells of interest."
keywords: "RNA-seq, retina, RPE, ocular, eye, Snakemake, singlecell, scRNA, single-cell, singlecell, scPOP, R"
---

```{r Setup..., message=FALSE, warning=FALSE, include=FALSE}
#knitr::opts_chunk$set(fig.pos = 'p') # Places figures on their own pages
knitr::opts_chunk$set(out.width = '100%', dpi=300)
library(tidyverse)
library(citr)
library(cowplot)
library(ggrepel)
library(colorspace)
library(flextable)
library(captioner)
library(pool)
library(RSQLite)
library(formattable)
# setup caption-ing
fig_cap <- captioner("Figure")
supFig_cap <- captioner("Supplemental Figure")
tab_cap <- captioner("Table")
supTab_cap <- captioner("Supplemental Table")

scEiaD_2020_v01 <- dbPool(drv = SQLite(), dbname = "~/data/massive_integrated_eye_scRNA/MOARTABLES__anthology_limmaFALSE___Mus_musculus_Macaca_fascicularis_Homo_sapiens-0-2000-counts-TabulaDroplet-batch-scVI-8-0.001-500-0.6.sqlite", idleTimeout = 3600000)

meta_filter <- fst::read_fst('~/git/plaeApp/inst/app/www/meta_filter.fst') %>% as_tibble()

# cells before QC removal (mito / likely doublets)
load('~/data/massive_integrated_eye_scRNA/cell_info_labelled.Rdata')

# meta data before doublet removal
load('~/data/massive_integrated_eye_scRNA/Mus_musculus_Macaca_fascicularis_Homo_sapiens__n_spec_genes-0__n_features2000__counts__TabulaDroplet__batch__scVI__dims8__preFilter__mindist0.001__nneighbors500.umap.Rdata')
umapRef <- umap

methods <- factor(c('scArches', 'bbknn','insct','magic', 'scVI','CCA', 'scanorama', 'harmony', 'fastMNN', 'combat', 'none', 'liger'), levels = c('bbknn','CCA','combat','fastMNN','harmony','insct','liger','magic','scanorama','scArches','scVI', 'none')) 

gse <- read_tsv('~/git/massive_integrated_eye_scRNA/data/GEO_Study_Level_Metadata.tsv')

qc <- readr::read_tsv('~/data/massive_integrated_eye_scRNA/QC.tsv.gz')
```


# Introduction
## Explosion of single cell transcriptomic atlases
The  recent introduction of relatively low cost and high throughput single cell sequencing technlogy has led to an explosion of research across many fields. As of 2020, over 30 million cells have been sequenced across over 1,000 studies (Svennson Pachter http://www.nxn.se/single-cell-studies). As of November 2020, the average size of each individual dataset is near 100,000 cells. Some of the seminal work in the high throughput single cell transcriptomics field (Macosko et al) have used the retina. As of late 2020, over a dozen published studies containing over a million cells have used single cell technology to profile cell type specific gene expression patterns, cell fate trajectory, tissue and cell differentiation, and disease perturbation. 

## Many researchers use processed count tables for methods development and hypothesis testing

While the gene - cell count tables are often made available in repositories like the Gene Expression Omnibus (GEO), there are no requirements to uniformly process the data. This means the count tables cannot be used in cross-study comparisons as even small differences in the computational pipeline (aligner, transcriptome reference, etc.) create study-specific effects. This issue can be addressed only by re-quantifying the data in a uniform environment. Fortunately, due to the continued development of computationally light-weight gene quantification tools into the single-cell space (kbtools, alevin-something), re-quantification does not require massive compute resources. 

Unfortunately, there still remain study specific batch effects due to the variety in single cell technologies used and variation in tissue handling and processing across different scientific groups. The single cell community has recognized that removal of these technical (also referred to as batch) effects is a critical issue and have independently developed many tools. It remains unclear which tool(s) is optimal for a particular dataset. 

<!-- Study specific web portals, should they exist, are silos which only contain the single cell transcriptomes related to their study. There are notable efforts to create web portals (UCSC Single Cell Browser, EBI Single Cell Expression Atlas) that allow for GUI-based viewing of data for most publicly available datasets. However, these again have the shortcoming as the study-specific web portals discussed above in that you still cannot directly compare between independent studies. In these situations it is impossible to compare directly across datasets. When exploring a gene expression pattern, it is crucial to have reproducibility across disparate groups to enhance confidence that effect is biological as bench experiments require large investments in time.  -->

<!-- ## Aggregation of results to create meta-atlas important -->

<!-- While these groups has leveraged this new technology to [do stuff], there is a large chasm to cross the enable outside groups to use the data in new ways. Several of these papers are accompanied by reactive web apps, which allow for relatively quick checks of gene expression across cell type or cluster assignment. However these web apps have minimal to no data exploration tools and are slow to operate, especially with the larger datasets.  -->

<!-- While lightly processed counts data is often provided, differences in both single cell technology (e.g. droplet or well based) and bioinformatic processing choices make it impossible to simply concatenate the datasets together as differences found will likely be technical.  -->

## The meta-atlas

We propose that by re-processing publicly available raw single cell transcriptome data in a consistent bioinformatic framework and optimally using batch correction tools we can create a meta-atlas, which combines individual datasets into one unified (or meta-) atlas. As there are thousands of possible permutations of single cell tools, references, and parameter choices, we create our meta-atlas (which we refer to as the single cell eye in a disk or scEiaD) by benchmarking integration outcomes across multiple important single cell RNA-seq processing parameters (integration tool, number of hyper-variable genes, clustering resolution, etc). The benchmarking system we developed uses a wide variety of metrics and we make available to all via an R package scPOP (single-cell Pick Optimal Parameters). The scEiaD will be of utility to two communities: the ocular community who can search scEiaD for gene expression across many dimensions (e.g. cluster, cell type, study) and the computational community who can use this very large, well-curated dataset to test algorithms for compute efficiency and performance in a diverse environment. As we believe data re-use is a powerful and accessible means to drive forward research, we provide our meta-atlas code-base and propose general guidelines to optimally create custom meta-atlases.

# Results

## We identify `r meta_filter %>% pull(study_accession) %>% unique() %>% length() - 1` ocular scRNA datasets across 3 species

The first step in building a meta-atlas is identifying studies to draw the data from. We identified ocular single cell RNA seq (scRNA) studies by querying PubMed, the Sequence Read Archive (SRA), and the European Nucleotide Archive (ENA) for the inclusive terms "retina", "single cell", "scRNA", "ocular", "eye", "transcriptome" and then hand filtering the results to only keep normal (non-perturbed or mutagenized) data from single cell RNA-seq technology. As of September 2020 we identified `r meta_filter %>% pull(study_accession) %>% unique() %>% length() - 1` deposited datasets that have been published in `r meta_filter %>% pull(Citation) %>% unique() %>% length()` publications (`r fig_cap(name = 'fig1', display = 'cite')`). To provide a non-ocular reference we also downloaded the raw sequence data for the Tabula Muris project. 
In cases where the fastq file from the SRA was not processed properly, we acquired the original bam files (personal correspondence or the SRA) and re-extracted the fastq. After downloading all the data we had 5.7 TB across `r scEiaD_2020_v01 %>% tbl('metadata_filter') %>% pull(sample_accession) %>% unique() %>% length()` fastq file sets. 

```{r fig1, fig.width=13, fig.height=6, fig.cap=fig1_cap, echo=FALSE, message = FALSE}
fig1_cap <- fig_cap(name='fig1', caption =  'a. Counts of (study) accession, published papers, and batches for each scRNA technlogy, split by organism. b. Simplified directed workflow of major steps in scEiaD creation from raw counts to gene counts, benchmarking optimal integration methods (SnakePOP) to produce batch corrected latent dimensions (LatentDims), then downstream analysis outputs like clustering, differential gene testing (DiffTesting), and 2D UMAP visualization. c. Cell type counts extracted from published studies for the more common retina cell types, split by species. Count of study accessions for each species overlaid on bar plot.')

source('figs_and_tables/fig1.R')
plot_grid(plot_grid(a, c, align = 'v',  axis = 'lr', ncol = 1, labels = c('','c')), b,  nrow = 1, rel_widths = c(1.1,1), labels = c('a','b'))
```

## `r cell_info_labels %>% nrow() %>% as.integer() %>% format(., big.mark=",", scientific=FALSE)` cells before quality control

Gene-level counts were quantified with the kallisto bustool pseudo-aligner for both the droplet and well based samples. For the droplet-based technology data, the inflection point between empty and non-empty droplets was automatically determined using the DropletUtils barcodeRank function in R. After empty droplet removal, we had `r format(cell_info_labels %>% nrow() %>% as.integer() , big.mark=",", scientific=FALSE)` cells. We then created a Seurat v3 object, calculated the percent mitochondrial genes counts, and removed cells which had more than 10% mitochondrial reads across all gene counts. For the droplet-based data, we also removed cells which had more than 3000 unique genes detected as these are likely to be doublets. After these standard quality control steps we were left with `r format(umapRef %>% nrow, big.mark = ",", scientific=FALSE)` cells. 

## Extraction and curation of cell type labels

A core objective of many scRNA based studies is labeling distinct cell types. As this information is crucial to assess dataset integration and provide an accurate reference for user querying, we extracted individual cell labels with a combination of hand-searching the Gene Expression Omnibus, supplemental information from the publication, web resources (if an web app was made available), and personal correspondence. After normalizing cell type names, we obtained labels for `r meta_filter$CellType %>% table() %>% sum()` cells across `r meta_filter$CellType %>% table() %>% length()` cell types (`r supTab_cap(name = 'supTab1', display = 'cite')`). 

## Running `r methods %>% length()` tools in a Snakefile-based system

Removal of the technical effects from the data is a crucial step in assessing the biological patterns out of the a meta-dataset. A wide variety of tools have been designed for this function. As we were uncertain which would perform the best and it is difficult to manually assess performance we ran `r methods %>% length()` tools with a commonly used set of key parameters that affect integration: 2000 hyper variable genes, 8 and 30 latent dimensions returned, and the louvain clustering algorithm with 7 k-nearest neighbors

There are two key metrics which have to be balanced in order to optimize performance: batch mixing (the same cell types should be similar across independent studies) and cell type or cluster purity (where different cell types or clusters should be distinct). While these can be visually assessed by looking at marker gene expression across the 2D t-SNE or UMAP projection, it is more rigorous and scalable to quantify this process.

We define batch as being each biological sample and assume each study is at least one unique sample. We studied the metadata and methods of each study to determine if the particular study contained multiple unique biological samples. In the end we identified `r umapRef %>% pull(batch) %>% unique() %>% length()` batches across `r umapRef %>% pull(study_accession) %>% unique() %>% length()` deposited datasets in `r umapRef %>% left_join(gse) %>% pull(PMID) %>% unique() %>% length()` published papers.

## scPOP wraps several different methods at measuring integration performance. 

scPOP brings together the Local Simpson Index (LISI) and Average Silhouette Width (ASW) metrics from Harmony and kBET, respectively, and the Adjusted Rand Index (ARI), Normalized Mutual Information (NMI), and PCR metrics from scIB. The LISI and ASW were used to measure batch mixing (where lower is better), cell type mixing (higher is better), and cluster mixing (higher is better). NMI and ARI were used to assess the consistency of cell type to cluster assignment (where 1 is perfect correspondence between cluster and cell type). The PCR metric (higher is better) runs PCA before and after integration to assess how much variance is explained by the batch corrected dimensional space. 

To visualize the interplay between batch mixing and cell type distinction we plot the batch mixing LISI score (which has been multiplied by -1) was plotted on the y-axis (higher is better) against the cluster LISI on the x-axis (higher is better). The best performer on both metrics will be in the top right corner  (`r supFig_cap(name = 'supFig_benchmark', display = 'cite')`a). In the same manner we plot the silhouette metric (`r supFig_cap(name = 'supFig_benchmark', display = 'cite')` b). 

## scPOP reveals different priorities across the different batch integration methods

On one extreme we have combat, which merges together different batches very well, but also mixes together the distinct cell types (`r fig_cap(name = 'figBenchmark', display = 'cite')`a). The other extreme is running the pipeline without any batch integration method, where you see very distinct groups of cells, but each study also is a distinct group in the 2D UMAP space (`r fig_cap(name = 'figBenchmark', display = 'cite')`b). In this dataset we see that like combat, Harmony is weighed more towards batch mixing then cluster and cell type purity. Scanorama, fastMNN, and bbknn are more weighed to cleanly separate the clusters. With out scEiaD insct, scArches, and magic do not perform particularly well in batch mixing or cluster purity. 

## Different normalization methods alter integration performance

https://www.frontiersin.org/articles/10.3389/fgene.2020.00041/full
Another substantial area of uncertainty is expression normalization. A popular method (which we call "standard") that Seurat and anndata use by default is log normalization where the raw counts and divided by the sum counts for the cell, multiplied by a scaling factor, then log transformed. This helps make the data more normal in the distribution pattern, which is an assumption that some algorithms require. The scran normalization method groups cells into pools and normalizes across the pool summed counts instead of the individual cell counts. We also use the relatively simple square root (sqrt) normalization which replaces the log transformation with a square root. Library size (libSize) normalization omits the sqrt or log transformation. Finally some methods like scVI directly use the raw counts data for modelling the data. 

We see in the across the sumZScale metric, which we define as sumZScale = zscale(PCR) + zscale(LISI batch) + zscale(LISI cluster) + zscale(LISI cell type) + zscale(silhouette batch) + zscale(silhouette cluster) + zscale(silhouette cell type) + zscale(NMI) + zscale(ARI) that scVI overall performs best. As expected the libSize normalization which omits the log or square root scaling generally performs the worst (`r fig_cap(name = 'figBenchmark', display = 'cite')`c). We see that the remaining normalization approaches alters the batch correction performance, though the exact outcome differs across the different methods.  For users who have custom needs, the weights of each metrics can be customized in scPOP to prioritize batch mixing or cluster/cell type separation.

```{r figBenchmark, fig.width=13, fig.height=8, fig.cap=figBenchmark_cap, echo=FALSE, message = FALSE}
figBenchmark_cap <- fig_cap(name='figBenchmark', caption =  'a. Example of a method (combat) which has a high level of batch blending, but poor separation of cell types (colored by cell type). b. no batch correction cleanly separates cell types but does not mix batches (colored by study). c. sumZScale (higher is better) for each method across a variety of data normalizations. All methods shown here use 2000 HVG, louvain clustering, and 8 latent dimensions d. Optimal parameters for scVI methods across 1000 to 10000 HVGs and 4 to 100 latent dimensions.')

source('figs_and_tables/fig_umap_benchmarking.R')
plot_grid(
  plot_grid(
    umap_plot_maker(umapCombat), 
    umap_plot_maker(umapNone, color_against = 'study_accession'),
    rel_widths = c(1.3, 1),
    labels = c('a','b')), 
  plot_grid(
    zscore_sum_all_methods, 
    zscore_droplet_scVI_optimize,
    labels = c('c','d')),
  nrow = 2,
  rel_heights = c(1, 0.4)
)
```
## scVI has highest integration performance

scPOP finds that for our dataset scVI has the strongest performance, with a sum score of SOMETHING. The next highest performers are fastMNN, harmony, and scanorama. We do not claim that these results are general - rather we emphasize that for each (meta) atlas creation that scPOP or some similar unbiased approach where a range of several key parameters (e.g. HVG number) are tested in a quantitative framework. 

## Further optimization of scVI with grid search

To find the best set of parameters for integration and clustering we did a grid search across key parameters: hyper variable genes (HVG), latent dimensions, and k nearest neighbors. Using scPOP we found the optimal parameters to be 2000 HVG, 8 latent dimensions, and a resolution of 0.6 with the leiden algorithm implemented by XXXXX (`r fig_cap(name = 'figBenchmark', display = 'cite')`d). We also varied the UMAP projection values of nearest neighbors and minimum distance to qualitatively pick a 2D projection, selecting a minimum distance of 0.001 and 500 nearest neighbors. 

## move this to the methods
While we removed droplet-based cells with more than 3,000 transcripts as these are highly likely to be doublets (more than one cell per droplet) there are likely many more doublets. We ran DoubletDetect and scrublet and calculated the distribution of DoubletDetect and scrublet scores across all clusters and removed clusters with a DoubletDetect and scrublet score more than 4 standard deviations above the mean. This removed another `r format((umapRef %>% nrow() %>% as.integer()) - (meta_filter %>% nrow() %>% as.integer()), big.mark=",", scientific=FALSE)` cells, leaving `r format((meta_filter %>% nrow()) %>% as.integer(), big.mark=",", scientific=FALSE)` in total. 

## xgboost model built to label unknown cell types
To further study cell type specific expression patterning we needed to label the `r format(umapRef %>% nrow() - meta_filter %>% filter(!is.na(CellType)) %>% nrow(), big.mark=",", scientific=FALSE)` unlabelled cells. Traditionally this is done by clustering the cells, then using cell type specific markers to label the clusters. However, as we had hundreds of thousands of pre-labelled cells across `r umapRef %>% filter(!is.na(CellType)) %>% left_join(gse) %>% pull(PMID) %>% unique() %>% length()` studies (`r fig_cap(name = 'fig1', display = 'cite')`a) we leveraged the expert labels by building a xgboost-based machine learning model that used the half of the labeled cells as a training set (see methods for more details) to train a cell type predictor for scEiaD input. The trained model was used to predict the cell type assignments for all cells in scEiaD. In this manner we both label most cells (cells which cannot be assigned a cell type assignment with a confidence above 0.5 were left unlabelled) and correct a small number of probable mislabels in the truth set.    

~~ALLUVIAL PLOT AND REF UMAP FIGURE~~

## scEiaD UMAP projection blends batches and separates cell types
The 2D UMAP projection of the 8 scVI-calculated latent dimensional blends the `r meta_filter %>% pull(study_accession) %>% unique() %>% length() - 1` studies together while also maintaining distinct space for the `r meta_filter$CellType_predict %>% table() %>% length()` unique cell types. We see the neurogenic population from which the retinal cell types are derived near the center of the UMAP visualization. Cones and rods, which are the photoreceptors of the retina which are responsible for color and low-light vision, respectively, are near each other in the UMAP. The amacrine cells ....say more about cell type in space

## diff testing reveals....
pseudo and (haystack <- save for next paper?)

## Pan mouse as non-ocular reference


# Methods

## Quantification

kallisto/bustools

## Batch normalization
Tested:
- scVI
- CCA
- insct
- combat
- nothing
- magic
- scanorama
- harmony
- fastMNN
- scArches

## Grid search to pick optimal parameters

Varied:
- number of Hyper Variable Genes (HVG): 1000, 2000,5000,1000
- dims: 4,6,8,10,20,30,50,100
- normalization: scran, seurat standard, libSize, sqrt, counts (scVI only)
- nneighbors (UMAP viz only)
- knn: 5,7,10 (clustering)

benchmarked with:
- LISI
- ARI
- Silhouette
- PCA variance explained before/after correction

## Clustering
Louvain-jaccard as implemented by Seurat

## pseudobulk differential expression testing
- celltype (published) against remaining
- celltype (published) pairwise against celltype (published)
- species specific within celltype
- same for celltype (projected)
- same for cluster

# Conclusion


# Supplemental
```{r supFig1, fig.width=4, fig.height=6, out.width=300, fig.cap=supFig1_cap, echo=FALSE, message = FALSE}
supFig1_cap <- supFig_cap(name='supFig1_cap', caption =  'The left bar delineates the number of cells for each organism - technology combination. The right bar specifies the number of each cells in each post QC category. In silico doublets were identified with scrublet and DoubletDetector.')

source('figs_and_tables/sup_fig1__qc_failure.R')
supFig1_plot
```



```{r supFig_benchmark, fig.width=8, fig.height=8, out.width=300, fig.cap=supFig_benchmark_cap, echo=FALSE, message = FALSE}
supFig_benchmark_cap <- supFig_cap(name='supFig_benchmark', caption =  'Performance of the various batch correction tools across various benchmarking metrics. For the LISI and Silhouette plots in A, B higher means better batch mixing and further to the right means better cluster purity. For the ARI and NMI metrics (which reflects how well cluster matches with cell type) in C, D, higher means a better score.')

source('figs_and_tables/fig2__integrationPerf.R')
cowplot::plot_grid(
  cowplot::plot_grid(lisi + theme(legend.position="none"), silhouette, NULL, rel_widths = c(1,1,0.2), nrow = 1, labels = c('A' ,'B')), 
  cowplot::plot_grid(ari, nmi, legend, rel_widths = c(1,1, 0.2), ncol = 3, labels = c('C', 'D')), 
  nrow = 2)
```

```{r supTab1, fig.width=8, fig.height=14, out.width=300, fig.cap=supTab1_cap, echo=FALSE, message = FALSE}
supTab1_cap <- supTab_cap(name='supTab1_cap', caption =  'Counts for cell type labels. Published are the author created labels from the published datasets. Transferred area the cell labels that were transferred by a machine learning model onto the entire scEiaD dataset.')

source('figs_and_tables/sup_fig2__celltype_xgboost_labelling.R')
fontsize(ctTable, size = 8, part = 'all')
```


```{r supTab_benchmark, fig.width=8, fig.height=14, out.width=300, fig.cap=supTab1_cap, echo=FALSE, message = FALSE}
supTab_benchmark_cap <- supTab_cap(name='supTab_benchmark', caption =  'Counts for cell type labels. Published are the author created labels from the published datasets. Transferred area the cell labels that were transferred by a machine learning model onto the entire scEiaD dataset.')

source('figs_and_tables/sup_fig2__celltype_xgboost_labelling.R')
fontsize(ctTable, size = 8, part = 'all')
```